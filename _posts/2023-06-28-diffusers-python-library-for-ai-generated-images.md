---
layout: post
title: "Diffusers: Python Library for AI-Generated Images"
date: 2023-06-28 12:02:49 +0000
canonical_url: https://pub.towardsai.net/diffusers-python-library-for-ai-generated-images-9d5f4d17f622?source=rss-8c8a65726e4c------2
link: https://pub.towardsai.net/diffusers-python-library-for-ai-generated-images-9d5f4d17f622?source=rss-8c8a65726e4c------2
categories: [medium]
---

<h4>This article shows the basic usage of HuggingFace’s diffuser library, which is used for AI-generated images through code.</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/768/0*ZWB4N54_ndJvT0eK" /><figcaption>Image generated using Diffusers Pipeline with Code</figcaption></figure><h3>Introduction</h3><p>The Diffusers library maintained by HuggingFace is a go-to library for Generative AI that provides multiple stable diffusion pipelines for images, audio, and several other useful functionalities.</p><p>SD-WebUI is deployed on Automatic1111 and is available on <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">GitHub</a> for people who prefer GUI. However, for deployment purposes, GUI may not be the best option, and people have been using the diffuser library to deploy full-blown Generative AI applications. This article will showcase the setup and usage of a diffuser library for generating images using Stable Diffusion.</p><h3>Pre-requisites</h3><p>Firstly, you need to set up a fresh environment for the use of the diffuser library. A fresh environment is not a necessity but it helps avoid dependency clashes between pre-installed packages and versions required by diffusers and associated libraries. You can use either a Python virtual environment or a Conda environment.</p><p>In the new environment, run the following commands to set up the required packages.</p><pre>python -m pip install diffusers[torch]<br>python -m pip install transformers</pre><p>For documentation and codebase, refer to the GitHub links for <a href="https://github.com/huggingface/diffusers">Diffusers</a> and <a href="https://github.com/huggingface/transformers">Transformers</a> library. Both are currently maintained by HuggingFace, with new functionality and updates being pushed frequently.</p><h3>Code</h3><p>Basic text-to-image pipelines are provided by the Diffusers library. Other available pipelines, including Image-to-Image, Inpainting, and ControlNet, are used similarly. For this article, we will focus on the basic Stable Diffusion text-to-image pipeline.</p><h4>Import Relevant Libraries</h4><pre>from diffusers import StableDiffusionPipeline<br>import torch</pre><h4>Setup Hyperparameters and Constants</h4><pre>DEVICE = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;<br>PROMPT = &#39;hyperrealistic portrait of a man as astronaut, portrait, well lit, cyberpunk,&#39;<br>MODEL_ID = &#39;stabilityai/stable-diffusion-2-1&#39;</pre><p><em>Device</em> is used to select hardware devices. If an Nvidia GPU is detected in the system, it will be selected, which can speed up inference. In case a GPU is not available, inference can still be done on CPU hardware.</p><p>The <em>prompt</em> is the textual prompt that will be passed to the pipeline.</p><p><em>MODEL_ID</em> is the pre-trained model that will be fetched from HuggingFace. Multiple models are provided for Stable Diffusion available for use on HuggingFace Model Hub. Stable Diffusion 2.1 is the most recent release that provides 768x768 output results. Other models available are Stable Diffusion 1.5, and 2.0. Moreover, other fine-tuned models are also available that are trained for specific styles, such as anime or realistic images.</p><h4>Initialize Pipeline</h4><pre>pipe = StableDiffusionPipeline<br>            .from_pretrained(MODEL_ID, torch_dtype=torch.float16)<br>            .to(DEVICE)</pre><p>We use torch float16 precision instead of the default float32. Using Half precision, we can reduce GPU utilization that provides efficient inference.</p><p>The <strong>from_pretrained</strong> method fetches the pre-trained model from HuggingFace. It downloads and caches all required modules such as Text Encoder, Unet, and Variational AutoEncoder, and returns a StableDiffusionPipeline object.</p><p>We then convert to pipe to the dedicated hardware that is to be used for inference. It will be either GPU or CPU.</p><h4>Inference</h4><pre>result = pipe(PROMPT, num_inference_steps=50, guidance_scale=7).images[0]</pre><p>We pass the required parameters for the forward call of the StableDiffusionPipeline object. It returns an object of type StableDiffusionOutput that is built in the diffusers library. It contains a list of generated images. For our use case, we only fetch the first generated image.</p><p>The <strong>num_inference_steps </strong>argument sets the total denoising steps used. A higher number provides better results as the Unet can denoise an image for longer.</p><p>The <strong>guidance_scale </strong>argument controls the prompt conditioning on the output image. A lower guidance scale means the model pays less attention to the prompt, so the output may not reflect the prompt provided. However, the model has more creative freedom so the generated image can showcase more variations. A higher guidance scale focuses more on the prompt provided.</p><p>The generated image is a PIL Image object. So the Pillow library can be used to save or post-process the image further.</p><pre>result.save(&#39;result.png&#39;)</pre><h4>Output</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/768/0*GJJn8lztmbRw8Ojz" /><figcaption>Image generated using Diffusers Pipeline from Code</figcaption></figure><p>The result is a 768x768 dimension image based on the prompt provided. The exact dimension is based on the default size the pre-trained model was trained on. <strong>However, the dimension can be changed by using the height and width keyword argument during inference.</strong></p><h3>Complete Code</h3><pre>from diffusers import StableDiffusionPipeline<br>import torch<br><br>DEVICE = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;<br>PROMPT = &#39;hyperrealistic portrait of a man as astronaut, portrait, well lit, cyberpunk,&#39;<br>MODEL_ID = &#39;stabilityai/stable-diffusion-2-1&#39;<br><br>pipe = StableDiffusionPipeline.from_pretrained(MODEL_ID, torch_dtype=torch.float16).to(DEVICE)<br><br>result = pipe(PROMPT, num_inference_steps=50, guidance_scale=7).images[0]<br>result.save(&#39;result.png&#39;)</pre><h3>Conclusion</h3><p>The article highlighted the basic usage of the diffusers library that can be used for the deployment of applications related to Generative AI. There are multiple other pipelines available for different use cases. The API of each pipeline is similar, with simple changes in inference arguments making the required changes.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9d5f4d17f622" width="1" height="1" alt=""><hr><p><a href="https://pub.towardsai.net/diffusers-python-library-for-ai-generated-images-9d5f4d17f622">Diffusers: Python Library for AI-Generated Images</a> was originally published in <a href="https://pub.towardsai.net">Towards AI</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>