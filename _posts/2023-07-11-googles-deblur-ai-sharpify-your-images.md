---
layout: post
title: "Google’s Deblur AI: Sharpify your Images"
date: 2023-07-11 14:02:28 +0000
canonical_url: https://pub.towardsai.net/googles-deblur-ai-sharpify-your-images-3557c02ef1da?source=rss-8c8a65726e4c------2
link: https://pub.towardsai.net/googles-deblur-ai-sharpify-your-images-3557c02ef1da?source=rss-8c8a65726e4c------2
categories: [medium]
---

<h4>Say goodbye to blurry images. Google’s new technique unlocks the true potential of your phone’s camera.</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*WH00yo8aluM2QJmn" /><figcaption>Image by Author</figcaption></figure><h3>Introduction</h3><p>In our ever-evolving digital age, where capturing and sharing moments through photography has become an integral part of our lives, the frustration of ending up with blurry images can be disheartening. Whether it’s a cherished family photo, a breathtaking landscape, or a snapshot of a special occasion, blurry images can diminish the visual impact and rob us of the clarity we desire.</p><p>But fear not. Google’s new methodology provides a way to capture clear images straight from your phone. Most phones nowadays come with multiple cameras. Utilizing a single capture from two different cameras, Google uses learnable post-processing to refocus blurry images. By using the same scene captured using a <strong>Wide Angle (W) </strong>and <strong>Ultra-Wide Angle (UW) </strong>camera simultaneously, the method aims to combine both to obtain sharper results.</p><h3>Architecture</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*tPI75u5ppsM85AeB" /><figcaption>Image from <a href="https://defocus-control.github.io/">Paper</a></figcaption></figure><p>The <strong>DFNet </strong>model receives the wide-angle and ultra-wide-angle shots of the same scene as input, along with their defocus maps. The input and target defocus map represent the fuzziness of the original and output image, where each pixel value is proportional to the blurriness of the corresponding image pixel.</p><p>As the ultra-wide and ultra-wide angle images are extremely different, having varying depths of field, symmetry, focus, and colors, combining these images is not a trivial task. Therefore, Google introduces a learning-based methodology to stitch these images together.</p><p>The model takes the wide-angle image as the base image, where the ultra-wide image is used as a reference for high-frequency details. The model aims to blend both images, following the provided defocus maps, such that the output is a deblurred image.</p><blockquote>At test time, one can easily change the target defocus map, to deblur different parts of the image as required.</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*NKovLkPA9JBIlwCA" /><figcaption>Image from <a href="https://defocus-control.github.io/">Paper</a></figcaption></figure><p>As shown, to generate fully clear images, we can set the defocus map to all zeros. This motivates the model, to deblur all parts of the image. In other cases, specific portions of the image can be deblurred in accordance with the provided defocus map at test time.</p><h3>Results</h3><p>Achieving a PSNR and SSIM score of 29.78 and 0.898 respectively, the post-processing method outperforms previous methods in both qualitative and quantitative analysis.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1006/0*vKhLGQG2h4vuKI_N" /><figcaption>Image from <a href="https://defocus-control.github.io/">Paper</a></figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1016/0*geyp-4kxvnNLtV8v" /><figcaption>Image from <a href="https://defocus-control.github.io/">Paper</a></figcaption></figure><p>The results show the state-of-the-art results from previous methods, and Google’s DFNet, which attains better sharpness and details than its predecessors.</p><p>The model has potential uses in the domains of image refocus, depth of field (DoF) control and rerendering, and deblurring.</p><h3>Limitations</h3><h4>Need for Multiple Cameras</h4><p>The model uses Wide and Ultra-wide cameras which provide references for high-frequency details. Both images need to have different depths of fields, focusing on different parts of the scene. Images captured from identical cameras will not be able to replicate such results. Also, there is a major dependency on dual-camera phones, and image restoration is not possible with a single image input.</p><h4>Dataset Generation</h4><p>It is difficult to have a dataset of images captured using wide and ultra-wide angles that are widely available. It is also impossible to synthetically generate such datasets by adding a Gaussian blur to images that can replicate noise in real-world scenarios. To reduce the domain gap, the authors captured 100 image stacks for this method.</p><h4>Dependency on Pre-Existing Methods</h4><p>The data preprocessing part is a necessity to generate defocus maps, along with depth and occlusion masks. The preprocessing uses preexisting Optical Flow and Stereo Depth algorithms that are known to generate severe artifacts, resulting in the degradation of output images.</p><h3>Conclusion</h3><p>Blurriness Begone. Put an end to fuzzy images with Google’s recent advancement in image restoration. If incorporated into the AI behind phone cameras, we can see a picture-perfect world every day, right through our phones.</p><p>Consider reading the paper for a detailed understanding.</p><p>Paper: <a href="https://defocus-control.github.io/static/dc2_paper.pdf">https://defocus-control.github.io/static/dc2_paper.pdf</a></p><p>Follow me if you liked this article, and want to learn more about machine learning and recent advancements in the research community.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3557c02ef1da" width="1" height="1" alt=""><hr><p><a href="https://pub.towardsai.net/googles-deblur-ai-sharpify-your-images-3557c02ef1da">Google’s Deblur AI: Sharpify your Images</a> was originally published in <a href="https://pub.towardsai.net">Towards AI</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>