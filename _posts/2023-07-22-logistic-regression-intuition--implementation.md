---
layout: post
title: "Logistic Regression: Intuition & Implementation"
date: 2023-07-22 10:01:53 +0000
canonical_url: https://pub.towardsai.net/logistic-regression-intuition-implementation-e7a296caa225?source=rss-8c8a65726e4c------2
link: https://pub.towardsai.net/logistic-regression-intuition-implementation-e7a296caa225?source=rss-8c8a65726e4c------2
categories: [medium]
---

<h4>The math behind the Logistic Regression algorithm and implementation from scratch using Numpy.</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/640/1*rA80EhaJXnDsRvXR3YZ-Xw.png" /><figcaption>Image by Author</figcaption></figure><h3>Introduction</h3><p>Logistic Regression is a fundamental binary classification algorithm that can learn a decision boundary between two different sets of data attributes. In this article, we understand the theoretical aspect behind the classification model and implement it using Python.</p><h3>Intuition</h3><h4>Dataset</h4><p>Logistic Regression is a supervised learning algorithm so we have data feature attributes and their corresponding labels. The features are independent variables, denoted by <strong>X, </strong>and are represented in a one-dimensional array. The class labels, denoted by <strong>y, </strong>are either 0 or 1. Therefore, Logistic Regression is a binary classification algorithm.</p><p>If the data has d-featuers:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/79/0*a5Yf6dRnW7d2uv8Z" /></figure><h4>Cost Function</h4><p>Similar to Linear Regression, we have associated weights for the feature attributes and a bias value.</p><blockquote>Our aim is to find the optimal values for the weights and bias, to fit our data better.</blockquote><p>We have a weight value associated with each feature attribute and a single bias value.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/82/0*oMYtLO7B-N2quIm2" /></figure><p>We randomly initialize these values and optimize them using Gradient Descent. During training, we take a dot product between weights and features and add the bias term.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/221/0*NyqjZbk17EyObusd" /></figure><p>But because our target labels our 0 and 1, we use a non-linear Sigmoid function, represented by <strong>g</strong>, that pushes our values between the required range.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/151/0*1PdmEKhTEPBzKCLg" /></figure><p>The sigmoid function plotted is as follows:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Pa-vm4kyoEbRqqbu.png" /><figcaption>Image by Author</figcaption></figure><blockquote>Logistic Regression aims to learn weights and bias such that the values passed to the sigmoid function are positive for positive labels and negative for negative labels.</blockquote><p>To predict values in Logistic Regression, we pass our predictions to the sigmoid function. So, the prediction function is:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/213/0*eBrTy2Ao2Jp3dj8b" /></figure><p>Now that we have predictions from our model, we can compare them with the original target labels. The error between the predictions is calculated using the <strong>Binary Cross Entropy Loss</strong>. The loss function is as follows:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/415/0*nphdnLcJCfd1Txr9" /></figure><p>where y is the original target label, and p is the predicted value between [0,1]. The loss function aims to push the predicted values toward the actual target labels. If the label is 0 and the model predicts 0, the loss is 0. Similarly, if both predicted and target labels are 1, the loss is 0. Else, the model tries the converge to these values.</p><h4>Gradient Descent</h4><p>We use the Cost function and obtain the derivative concerning weights and bias. Using chain rule and mathematical derivation, the derivatives are as follows:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/287/0*KKWOlJAI65RDRUOS" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/249/0*aQQ19pdJnJhyhC_Q" /></figure><p>We obtain a scalar value that we can use to update the weights and biases.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/162/0*dEY69tSnSAThO8CU" /></figure><p>This updates the values against the loss gradient, so after multiple iterations, we gradually converge toward the optimal values of weights and biases.</p><p>During inference, we can then use the weights and bias values to obtain predictions.</p><h3>Implementation</h3><p>We utilize the mathematical formulas mentioned above to code the Logistic Regression model and will evaluate its performance on some benchmark datasets.</p><p>Firstly, we initialize the class and parameters.</p><pre>class LogisticRegression():<br>    def __init__(<br>            self,<br>            learning_rate:float=0.001,<br>            n_iters:int=10000<br>        ) -&gt; None:<br>        self.n_iters = n_iters<br>        self.lr = learning_rate<br>        <br>        self.weights = None<br>        self.bias = None</pre><p>We require the weights and bias values that will be optimized, so we initialize them here as object attributes. However, we can not set the size here as it depends on the data passed during training. Thus, we set them to None for now. The learning rate and number of iterations are hyperparameters that can be tuned for performance.</p><h4>Training</h4><pre>def fit(<br>        self,<br>        X : np.ndarray, <br>        y : np.ndarray<br>    ):<br>        n_samples, n_features = X.shape<br>        <br>        # Initialize Weights and Bias with Random Values<br>        # Size of Weights matirx is based on the number of data features<br>        # Bias is a scalar value<br>        self.weights = np.random.rand(n_features)<br>        self.bias = 0<br>        <br>        for iteration in range(self.n_iters):<br>            <br>            # Get predictions from Model<br>            linear_pred = np.dot(X, self.weights) + self.bias<br>            predictions = sigmoid(linear_pred)<br>            <br>            loss = predictions - y<br>            <br>            # Gradient Descent based on Loss<br>            dw = (1 / n_samples) * np.dot(X.T, loss)<br>            db = (1 / n_samples) * np.sum(loss)<br>        <br>            # Update Model Parameters<br>            self.weights = self.weights - self.lr * dw<br>            self.bias = self.bias - self.lr * db</pre><p>The training function initializes the weights and bias values. We then iterate over the dataset multiple times, optimizing these values towards convergence, such that the loss minimizes.</p><p>Based on the above equations, the sigmoid function is implemented as follows:</p><pre>def sigmoid(x):<br>    return 1 / (1 + np.exp(-x))</pre><p>We then use this function to generate predictions using:</p><pre>linear_pred = np.dot(X, self.weights) + self.bias<br>predictions = sigmoid(linear_pred)</pre><p>We calculate loss over these values and optimize our weights:</p><pre>loss = predictions - y<br>            <br># Gradient Descent based on Loss<br>dw = (1 / n_samples) * np.dot(X.T, loss)<br>db = (1 / n_samples) * np.sum(loss)<br><br># Update Model Parameters<br>self.weights = self.weights - self.lr * dw<br>self.bias = self.bias - self.lr * db</pre><h4>Inference</h4><pre>def predict(<br>        self,<br>        X : np.ndarray,<br>        threshold:float=0.5<br>    ):<br>        linear_pred = np.dot(X, self.weights) + self.bias<br>        predictions = sigmoid(linear_pred)<br>        <br>        # Convert to 0 or 1 Label <br>        y_pred = [0 if y &lt;= threshold else 1 for y in predictions]<br>        <br>        return y_pred</pre><p>Once we have fit our data during training, we can use the learned weights and bias to generate predictions similarly. The output of the model is in the range [0, 1], as per the sigmoid function. We can then use a threshold value such as 0.5. All values above this probability our tagged as positive labels and all values below this threshold our tagged as negative labels.</p><h4>Complete Code</h4><pre>import numpy as np<br><br><br>def sigmoid(x):<br>    return 1 / (1 + np.exp(-x))<br><br>class LogisticRegression():<br>    def __init__(<br>            self,<br>            learning_rate:float=0.001,<br>            n_iters:int=10000<br>        ) -&gt; None:<br>        self.n_iters = n_iters<br>        self.lr = learning_rate<br>        <br>        self.weights = None<br>        self.bias = None<br>    <br>    def fit(<br>        self,<br>        X : np.ndarray, <br>        y : np.ndarray<br>    ):<br>        n_samples, n_features = X.shape<br>        <br>        # Initialize Weights and Bias with Random Values<br>        # Size of Weights matirx is based on the number of data features<br>        # Bias is a scalar value<br>        self.weights = np.random.rand(n_features)<br>        self.bias = 0<br>        <br>        for iteration in range(self.n_iters):<br>            <br>            # Get predictions from Model<br>            linear_pred = np.dot(X, self.weights) + self.bias<br>            predictions = sigmoid(linear_pred)<br>            <br>            loss = predictions - y<br>            <br>            # Gradient Descent based on Loss<br>            dw = (1 / n_samples) * np.dot(X.T, loss)<br>            db = (1 / n_samples) * np.sum(loss)<br>        <br>            # Update Model Parameters<br>            self.weights = self.weights - self.lr * dw<br>            self.bias = self.bias - self.lr * db<br>        <br>    <br>    def predict(<br>        self,<br>        X : np.ndarray,<br>        threshold:float=0.5<br>    ):<br>        linear_pred = np.dot(X, self.weights) + self.bias<br>        predictions = sigmoid(linear_pred)<br>        <br>        # Convert to 0 or 1 Label <br>        y_pred = [0 if y &lt;= threshold else 1 for y in predictions]<br>        <br>        return y_pred</pre><h3>Evaluation</h3><pre>import numpy as np<br>import matplotlib.pyplot as plt<br><br>from sklearn.model_selection import train_test_split<br>from sklearn.datasets import load_breast_cancer<br>from sklearn.metrics import accuracy_score<br><br>from model import LogisticRegression<br><br><br>if __name__ == &quot;__main__&quot;:<br>    data = load_breast_cancer()<br>    X, y = data.data, data.target<br>    <br>    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)<br>    <br>    model = LogisticRegression()<br>    model.fit(X_train, y_train)<br>    y_pred = model.predict(X_test)<br>    score = accuracy_score(y_pred, y_test)<br>    <br>    <br>    print(score)</pre><p>We can use the above script to test our Logistic Regression model. We use the Breast Cancer dataset from Scikit-Learn for training. We can then compare our predictions with the original labels.</p><blockquote>Carefully fine-tuning some of the hyperparameters gave me an accuracy score of above 90%.</blockquote><p>We can use different dimensionality reduction techniques such as PCA, to visualize the decision boundary. After reducing our features to two dimensions, we obtain the following decision boundary.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/640/1*rA80EhaJXnDsRvXR3YZ-Xw.png" /><figcaption>Image by Author</figcaption></figure><h3>Conclusion</h3><p>In conclusion, this article explored the mathematical intuition of Logistic Regression and demonstrated its implementation using NumPy. Logistic Regression is a valuable classification algorithm that utilizes the sigmoid function and gradient descent to find an optimal decision boundary for binary classification.</p><p>For code and implementation, refer to this <a href="https://github.com/MuhammadArham-43/PytorchImplementations">GitHub repo</a>. Follow me for more articles on deep learning architectures and research advancements.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e7a296caa225" width="1" height="1" alt=""><hr><p><a href="https://pub.towardsai.net/logistic-regression-intuition-implementation-e7a296caa225">Logistic Regression: Intuition &amp; Implementation</a> was originally published in <a href="https://pub.towardsai.net">Towards AI</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>