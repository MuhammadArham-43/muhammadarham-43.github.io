---
layout: post
title: "PyTorch Custom Datasets: Making your own Dataloaders"
date: 2023-06-28 03:09:58 +0000
canonical_url: https://medium.com/@arhamm40182/pytorch-custom-datasets-making-your-own-dataloaders-9b270087cb7e?source=rss-8c8a65726e4c------2
link: https://medium.com/@arhamm40182/pytorch-custom-datasets-making-your-own-dataloaders-9b270087cb7e?source=rss-8c8a65726e4c------2
categories: [medium]
---

<h4>Data is an integral part of Machine Learning but not all data sources are publicly available. For custom datasets, we need data loaders that are separately implemented from the training code.</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Ygg3MfbebUgIKtuV" /><figcaption>Photo by <a href="https://www.pexels.com/photo/two-white-printer-papers-near-macbook-on-brown-surface-590016/">Lukas from Pexels</a></figcaption></figure><h3>Introduction</h3><p>To learn the basics of custom data loaders, we implement an MNIST data loader in this article. We do this for a classification task where we have an image and a corresponding label for each image. We need to provide code on how to load and preprocess data. This can be implemented independently of any model allowing re-usability and abstraction.</p><h3>Dataset Abstract Class</h3><p>Any data loader we implement must inherit from the <strong>torch.utils.data.Dataset </strong>class that is provided by PyTorch for this very purpose. We have to override the <strong>__len__ </strong>and <strong>__getitem__</strong> methods that are internally used by PyTorch dataloaders to fetch data at runtime.</p><p>Any custom dataset implementation must follow this basic structure.</p><pre>from torch.utils.data import Dataset<br><br><br>class CustomDataset(Dataset):<br> def __init__(self) -&gt; None:<br>     super(CustomDataset, self).__init__()<br><br> def __len__():<br>     pass<br><br> def __getitem__(self, index):<br>     pass</pre><p>In the <em>__init__ </em>function, we load all files and required data that is necessary for loading a specific instance. For example, we may have a CSV file containing image paths and their corresponding labels. We load only the CSV file as it is sufficient to load each image and its label at runtime. <strong>We do not load all of the data during initialization as it is done at runtime using the __getitem__ function.</strong> This saves space as we do not have to load all of the data at once on GPU or CPU memory.</p><p>The <em>__len__</em> function returns a single integer value that represents the total data instances. We can return the total number of images or the total rows of a pandas data frame. The output of the <em>__len__</em> function acts as the upper bound for the index parameter passed to the <em>__getitem__</em> function so we do not face the IndexOutOfBound runtime exception.</p><p>The <em>__getitem__</em> function can return any number of values as required. For the classification task, we return both the image and label. For other tasks, we may only require the image or we may require more than two values. The loading of a specific data instance is done with the <em>__getitem__</em> function. We are provided an index, and we provide code to load the values at that index from the data files.</p><h3>Implementation: MNIST Custom Dataset</h3><h4>Imports</h4><pre>from torch.utils.data import Dataset, DataLoader<br>from torchvision.transforms import ToTensor, Normalize, Compose, Resize<br>from torchvision.datasets import MNIST</pre><p>Torchvision Datasets is a PyTorch library that provides helper classes to download datasets. We use the MNIST class that will allow using the MNIST dataset.</p><h4>Initialization</h4><pre>class MNISTDataset(Dataset):<br> def __init__(self, train: bool = True, output_size=(227,227)) -&gt; None:<br>     super(MNISTDataset, self).__init__()<br>     self.mnist = MNIST(<br>         root=&quot;data&quot;,<br>         train=train,<br>         download=True,<br>         transform=Compose([Resize(output_size), ToTensor(), Normalize(mean=(0.5,), std=(0.5,))]),)</pre><p>During initialization, the MNIST class downloads the data to the root directory we provide as a parameter. Also, we can define custom transforms that preprocess the data. If we are using locally stored data, we process data in the <em>__getitem__</em> function. Here, we resize an image as required, normalize it, and convert it to a PyTorch tensor.</p><h4>Loading Data</h4><pre>def __len__(self):<br>     return len(self.mnist)<br><br> def __getitem__(self, index):<br>     img, label = self.mnist[index]<br>     return img, label</pre><h4>Dataloader</h4><p>PyTorch dataloaders create batches of data and provide an iterator over the data. It requires a torch.utils.data.Dataset class as a parameter. As we have inherited from the same class, we can now use this custom dataset class to create a data loader.</p><pre>dataset = MNISTDataset(train=True)<br>mnist_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)</pre><h3>Conclusion</h3><p>This provides a clean interface for using various datasets. We implement each dataset class independently and can reuse it with the PyTorch dataloaders with any deep learning architecture.</p><p>If we visualize our data, we achieve the following results.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*61uoGh-qL8jW5mIM" /><figcaption>Image by Author</figcaption></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9b270087cb7e" width="1" height="1" alt="">